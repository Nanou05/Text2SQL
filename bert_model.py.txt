import pandas as pd
import transformers
from transformers import  BertModel, BertTokenizer, AdamW
import torch
from datasets import load_dataset
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
from torch.nn import functional as F


dataset = load_dataset("wikisql")

train_data = dataset['train'].to_pandas()
test_data = dataset['test'].to_pandas()

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')


class TextToSQLBERT(nn.Module):
    def __init__(self, bert_model='bert-base-uncased'):
        super(TextToSQLBERT, self).__init__()
        self.bert = BertModel.from_pretrained(bert_model)
        self.tokenizer = BertTokenizer.from_pretrained(bert_model)
        
        #ajout des tokens SQL au vocabulaire
        sql_tokens = [
            'SELECT', 'FROM', 'WHERE', 'AND', 'OR', 'IN', 'NOT',
            'COUNT', 'SUM', 'AVG', 'MIN', 'MAX', 'GROUP BY',
            'ORDER BY', 'DESC', 'ASC', 'HAVING', 'LIMIT',
            '=', '<', '>', '<=', '>=', '!='
        ]
        
        #ajout des tokens au tokenizer
        special_tokens_dict = {'additional_special_tokens': sql_tokens}
        num_added_tokens = self.tokenizer.add_special_tokens(special_tokens_dict)
        
        # Redimensionnement des embeddings pour inclure les nouveaux tokens
        self.bert.resize_token_embeddings(len(self.tokenizer))
        
        self.decoder = nn.Sequential(
            nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(self.bert.config.hidden_size, len(self.tokenizer))
        )
        
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        sequence_output = outputs.last_hidden_state
        logits = self.decoder(sequence_output)
        return logits


def generate_sql_query(model, question, max_length=128):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    model.eval()
    
    # Préparation de la question
    input_text = f"convert to SQL: {question}"
    encoded_input = model.tokenizer(
        input_text,
        return_tensors='pt',
        padding=True,
        truncation=True,
        max_length=max_length
    )
    
    input_ids = encoded_input['input_ids'].to(device)
    attention_mask = encoded_input['attention_mask'].to(device)
    
    generated_tokens = []
    
    with torch.no_grad():
        # Génération token par token
        for _ in range(max_length):
            outputs = model(input_ids, attention_mask)
            next_token_logits = outputs[:, -1, :]
            
            # Application du top-k sampling
            top_k = 5
            indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]
            next_token_logits[indices_to_remove] = float('-inf')
            
            probs = F.softmax(next_token_logits / 0.7, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
            
            generated_tokens.append(next_token.item())
            
            #màj des input_ids pour la prochaine itération
            input_ids = torch.cat([input_ids, next_token], dim=1)
            attention_mask = torch.cat([attention_mask, torch.ones((1, 1), device=device)], dim=1)
            
            # Arrêt si on génère le token de fin
            if next_token.item() == model.tokenizer.sep_token_id:
                break
    
    #décodage des tokens en texte
    sql_query = model.tokenizer.decode(generated_tokens, skip_special_tokens=True)
    
    # Post-traitement pour assurer une requête SQL valide
    sql_query = clean_sql_query(sql_query)
    
    return sql_query


def clean_sql_query(sql_query):
    """Nettoie et formate la requête SQL générée"""
    # Suppression des espaces multiples
    sql_query = ' '.join(sql_query.split())
    
    # S'assurer que la requête commence par SELECT
    if not sql_query.upper().startswith('SELECT'):
        sql_query = 'SELECT ' + sql_query
    
    # Correction des espaces autour des opérateurs
    operators = ['=', '<', '>', '<=', '>=', '!=']
    for op in operators:
        sql_query = sql_query.replace(op, f' {op} ')
    
    return sql_query

# train the model
def train_model(model, train_dataloader, val_dataloader=None, epochs=10):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)
    criterion = nn.CrossEntropyLoss(ignore_index=model.tokenizer.pad_token_id)
    
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        
        for batch in train_dataloader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            optimizer.zero_grad()
            outputs = model(input_ids, attention_mask)
            
            #Calcul de la loss
            loss = criterion(outputs.view(-1, outputs.shape[-1]), labels.view(-1))
            loss.backward()
            
            #gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            total_loss += loss.item()
        
        avg_loss = total_loss / len(train_dataloader)
        print(f"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}")


def prepare_example_data(question, sql):
    # Préparation des données d'entraînement
    model = TextToSQLBERT()
    
    input_text = f"convert to SQL: {question}"
    target_text = sql
    
    # Tokenization
    inputs = model.tokenizer(
        input_text,
        padding=True,
        truncation=True,
        return_tensors='pt'
    )
    
    with model.tokenizer.as_target_tokenizer():
        labels = model.tokenizer(
            target_text,
            padding=True,
            truncation=True,
            return_tensors='pt'
        )['input_ids']
    
    return {
        'input_ids': inputs['input_ids'],
        'attention_mask': inputs['attention_mask'],
        'labels': labels
    }


if __name__ == "__main__":
    model = TextToSQLBERT()
    
    # Test de génération
    question = "What is the population of New York City?"
    sql_query = generate_sql_query(model, question)
    print(f"Question: {question}")
    print(f"Generated SQL: {sql_query}")
